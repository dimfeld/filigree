{% if file_upload %}
//! Object storage functionality for {{model_name}}
#![allow(unused_imports,  unused_variables, dead_code)]

use bytes::Bytes;
use error_stack::ResultExt;
use filigree::{
    storage::{Storage, StorageError},
    uploads::{self, UploadInspector, UploadInspectorError},
};
use futures::stream::Stream;
use sqlx::PgConnection;

use crate::{
    auth::Authed,
    error::Error,
    server::ServerState
};
use super::{ {{struct_base}}, {{struct_base}}UpdatePayload, {{id_type}} };
{{imports}}

/// Apply the storage key template
pub fn generate_object_key(auth: &Authed, id: {{id_type}}, filename: &str) -> String {
    {{file_upload.filename_template_function_body}}
}

pub fn get_storage(state: &ServerState) -> &Storage {
    &state.storage.{{file_upload.bucket}}
}

pub async fn upload_stream<E>(
    state: &ServerState,
    auth: &Authed,
    tx: &mut PgConnection,
    parent_id: {{belongs_to_field.rust_type}},
    id: Option<{{id_type}}>,
    filename: Option<String>,
    key: Option<String>,
    limit: Option<usize>,
    body: impl Stream<Item=Result<Bytes, E>> + Unpin
) -> Result<{{struct_base}}, error_stack::Report<Error>>
where
    StorageError: From<E>,
    UploadInspectorError: From<E>,
{
    let storage = get_storage(state);

    {% if file_upload.many %}
    let id = id.unwrap_or_else({{id_type}}::new);
    {% else %}
    // Get the actual ID first, since in the single-child case the ID won't change
    // if the object already exists.
    let existing_id = sqlx::query_scalar!(
        r##"SELECT id: "id: {{id_type}}" FROM {{table}} WHERE {{belongs_to_field}} = $1"##,
        parent_id
    )
        .fetch_optional(&mut *tx)
        .await
        .change_context(Error::Db)?;

    let id = existing_id.or(id).unwrap_or_else({{id_type}}::new);
    {% endif %}

    let file_storage_key = key.unwrap_or_else(|| generate_object_key(auth, id, filename.as_deref().unwrap_or_default()));

    {% if file_upload.record_size -%}
    let mut file_size = uploads::UploadSize::new(limit);
    {%- endif %}
    {% if file_upload.hash -%}
    let mut hasher = uploads::UploadHasher::<{{file_upload.hash.hasher}}>::new();
    {%- endif %}

    storage.save_and_inspect_request_body(&file_storage_key, body, |chunk| {
        {% if file_upload.record_size -%}
        file_size.inspect(chunk)?;
        {%- endif %}
        {% if file_upload.hash -%}
        hasher.inspect(chunk)?;
        {%- endif %}
        Ok::<(), UploadInspectorError>(())
    })
    .await
    .change_context(Error::Upload)?;

    let db_payload = {{struct_base}}UpdatePayload {
        id: Some(id),
        {{belongs_to_field.name}}: parent_id,
        file_storage_key,
        file_storage_bucket: "{{file_upload.bucket}}".to_string(),
        {% if file_upload.record_filename -%}
        file_original_name: filename,
        {%- endif %}
        {% if file_upload.hash -%}
        file_hash: Some(hasher.finish().to_vec()),
        {%- endif -%}
        {% if file_upload.record_size -%}
        file_size: Some(file_size.finish() as i64),
        {%- endif %}
        ..Default::default()
    };

    let result = super::queries::upsert_with_parent(
        tx,
        auth.organization_id,
        true,
        parent_id,
        &db_payload
    )
        .await?;

    Ok(result)
}

pub async fn upload(
    state: &ServerState,
    auth: &Authed,
    tx: &mut PgConnection,
    parent_id: {{belongs_to_field.rust_type}},
    id: Option<{{id_type}}>,
    filename: Option<String>,
    key: Option<String>,
    limit: Option<usize>,
    body: Bytes
) -> Result<{{struct_base}}, error_stack::Report<Error>> {

    let file_size = body.len();
    if let Some(limit) = limit {
        if file_size > limit {
            return Err(
                error_stack::Report::new(UploadInspectorError::FileSizeTooLarge)
                )
                .change_context(Error::Upload);
        }
    }

    {% if file_upload.hash %}
    let b = body.clone();
    let hash = tokio::task::spawn_blocking(move || {
        let mut hasher = uploads::UploadHasher::<blake3::Hasher>::new();
        hasher.inspect(&b).ok();
        hasher.finish().to_vec()
    })
    .await
    .change_context(Error::Upload)?;
    {% endif %}

    {% if file_upload.many %}
    let id = id.unwrap_or_else({{id_type}}::new);
    {% else %}
    // Get the actual ID first, since in the single-child case the ID won't change
    // if the object already exists.
    let existing_id = sqlx::query_scalar!(
        r##"SELECT id: "id: {{id_type}}" FROM {{table}} WHERE {{belongs_to_field}} = $1"##,
        parent_id
    )
        .fetch_optional(&mut *tx)
        .await
        .change_context(Error::Db)?;

    let id = existing_id.or(id).unwrap_or_else({{id_type}}::new);
    {% endif %}

    let file_storage_key = key.unwrap_or_else(|| generate_object_key(auth, id, filename.as_deref().unwrap_or_default()));

    let db_payload = {{struct_base}}UpdatePayload {
        id: Some(id),
        {{belongs_to_field.name}}: parent_id,
        file_storage_key: file_storage_key.clone(),
        file_storage_bucket: "{{file_upload.bucket}}".to_string(),
        {% if file_upload.record_filename -%}
        file_original_name: filename,
        {%- endif %}
        {% if file_upload.hash -%}
        file_hash: Some(hash),
        {%- endif %}
        {% if file_upload.record_size -%}
        file_size: Some(file_size as i64),
        {%- endif %}
        ..Default::default()
    };

    let result = super::queries::upsert_with_parent(
        tx,
        auth.organization_id,
        true,
        parent_id,
        &db_payload
    ).await?;

    let storage = get_storage(state);
    storage
        .put(&file_storage_key, body)
        .await
        .change_context(Error::Upload)?;

    Ok(result)
}

/// Delete an object given the storage key
pub async fn delete_by_key(state: &ServerState,  key: &str) -> Result<(), error_stack::Report<Error>> {
    let storage = get_storage(state);
    storage.delete(key).await.change_context(Error::Storage)?;
    Ok(())
}

/// Delete a file from the database and from object storage.
pub async fn delete_by_id(
    state: &ServerState,
    auth: &Authed,
    tx: &mut PgConnection,
    parent_id: {{belongs_to_field.rust_type}},
    id: {{id_type}}
) -> Result<(), error_stack::Report<Error>> {

    let storage_key = get_storage_key_by_id(state, auth, &mut *tx, id).await?;
    let deleted = super::queries::delete_with_parent(&mut *tx, auth, parent_id, id).await?;

    if deleted {
        delete_by_key(state, &storage_key).await?;
    }

    Ok(())
}

pub async fn get_storage_key_by_id(
    state: &ServerState,
    auth: &Authed,
    tx: &mut PgConnection,
    id: {{id_type}}
) -> Result<String, error_stack::Report<Error>> {
    let storage_key = super::queries::get(
        &mut *tx,
        auth,
        id
    )
        .await?
        .file_storage_key;
    Ok(storage_key)
}
{% endif %}
